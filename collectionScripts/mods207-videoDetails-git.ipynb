{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcbd0b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from googleapiclient.errors import HttpError\n",
    "import urllib.parse as p\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from google.auth.exceptions import RefreshError\n",
    "import logging\n",
    "import pandas as pd \n",
    "import warnings \n",
    "\n",
    "logging.basicConfig(level=logging.WARNING, filename='logging.txt')\n",
    "#logging.basicConfig(filename='logging.txt')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "SCOPES = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d385a96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def youtube_authenticate():\n",
    "    os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "    \n",
    "             ## you need a google developper account and install youtube api to the api key \n",
    "    client_secrets_file = r'code_secret_client.json' # change if you ran it on your personal environment\n",
    "\n",
    "    creds = None\n",
    "    # the file token.pickle stores the user's access and refresh tokens, and is\n",
    "    # created automatically when the authorization flow completes for the first time\n",
    "    if os.path.exists(\"token.pickle\"):\n",
    "        with open(\"token.pickle\", \"rb\") as token:\n",
    "            creds = pickle.load(token)\n",
    "    # if there are no (valid) credentials availablle, let the user log in.\n",
    "    if creds and creds.refresh_token:\n",
    "        try:\n",
    "            creds.refresh(Request())\n",
    "        except RefreshError:\n",
    "            logger.error(\"Credentials could not be refreshed, possibly the authorization was revoked by the user.\")\n",
    "            os.unlink('token.pickle')\n",
    "            return\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\n",
    "            client_secrets_file, SCOPES)\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open('token.pickle', 'wb') as token:\n",
    "        pickle.dump(creds, token)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "    return build(api_service_name, api_version, credentials=creds)\n",
    "\n",
    "# authenticate to YouTube API\n",
    "youtube = youtube_authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a20fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_id_by_url(url):\n",
    "    \"\"\"\n",
    "    Return the Video ID from the video `url`\n",
    "    \"\"\"\n",
    "    # split URL parts\n",
    "    parsed_url = p.urlparse(url)\n",
    "    # get the video ID by parsing the query of the URL\n",
    "    video_id = p.parse_qs(parsed_url.query).get(\"v\")\n",
    "    if video_id:\n",
    "        return video_id[0]\n",
    "    else:\n",
    "        raise Exception(f\"Wasn't able to parse video URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca1b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(youtube, **kwargs):\n",
    "    return youtube.videos().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        **kwargs\n",
    "    ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5040201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_video_infos(video_response, df):\n",
    "    items = video_response.get(\"items\")[0]\n",
    "    # get the snippet, statistics & content details from the video response\n",
    "    snippet         = items[\"snippet\"]\n",
    "    statistics      = items[\"statistics\"]\n",
    "    content_details = items[\"contentDetails\"]\n",
    "    id_vid          = items['id']\n",
    "    # get infos from the snippet\n",
    "    channel_title = snippet[\"channelTitle\"]\n",
    "    title         = snippet[\"title\"]\n",
    "    description   = snippet[\"description\"]\n",
    "    publish_time  = snippet[\"publishedAt\"]\n",
    "    if \"tags\" in snippet : \n",
    "        key_words = snippet[\"tags\"]\n",
    "    else :\n",
    "        key_words = []\n",
    "        \n",
    "    category      = snippet[\"categoryId\"]\n",
    "    # get stats infos\n",
    "    if \"commentCount\" in statistics : \n",
    "        comment_count = statistics[\"commentCount\"]\n",
    "    else : \n",
    "        comment_count = -1\n",
    "\n",
    "    if \"likeCount\" in statistics :\n",
    "        like_count = statistics[\"likeCount\"]\n",
    "    else :\n",
    "        like_count = -1\n",
    "        \n",
    "    favorite_count = statistics[\"favoriteCount\"]\n",
    "    view_count     = statistics[\"viewCount\"]    \n",
    "    # get duration from content detail  \n",
    "    duration   = content_details[\"duration\"]\n",
    "    definition = content_details[\"definition\"]\n",
    "    caption    = content_details[\"caption\"]\n",
    "    licensed   = content_details[\"licensedContent\"]\n",
    "    # duration in the form of something like 'PT5H50M15S'\n",
    "    # parsing it to be something like '5:50:15'\n",
    "    parsed_duration = re.search(f\"PT(\\d+H)?(\\d+M)?(\\d+S)\", duration)\n",
    "    \n",
    "    duration_str = \"\"\n",
    "    if parsed_duration != None :\n",
    "        for d in parsed_duration.groups():\n",
    "            if d:\n",
    "                duration_str += f\"{d[:-1]}:\"\n",
    "        duration_str = duration_str.strip(\":\")\n",
    "    \n",
    "    df = df.append({\"id\" : id_vid, \"Title\": title, \"Description\" : description,\n",
    "                    \"Channel Title\" : channel_title, \"Publish time\" : publish_time,\n",
    "                    \"Tags\": key_words, \"Category\" : category, \"Duration\" : duration_str,\n",
    "                    \"Number of comments\" : comment_count, \"Number of likes\" : like_count,\n",
    "                    \"Number of views\" : view_count, \"Number of favorites\" : favorite_count,\n",
    "                    \"Video quality\" : definition, \"Licensed\" : licensed} ,   ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e789e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_channel_videos(path, channel):\n",
    "    \n",
    "    channel_csv = pd.read_csv(path + channel)\n",
    "    channel_csv.drop_duplicates(inplace=True)  # drop ducplicates \n",
    "    channel_csv.dropna(inplace=True)  # drop nan\n",
    "    videos = channel_csv['videos'].apply(lambda x : str(x)) # all video urls are an str type\n",
    "    videos = videos[videos.apply(lambda x : \"https://www.youtube.com/watch?v\" in x)] # keep only videos\n",
    "    video_ids = videos.apply(lambda x : get_video_id_by_url(x)) # extract video id\n",
    "    \n",
    "    error = None\n",
    "    df = pd.DataFrame() \n",
    "    for video_id in video_ids :\n",
    "        try :\n",
    "            response = get_video_details(youtube, id=video_id)\n",
    "            df = save_video_infos(response, df)\n",
    "            #print(\"got details for\", video_id)\n",
    "            \n",
    "        except Exception as inst:\n",
    "            print(\"Error in video id \", video_id)\n",
    "            error = inst\n",
    "            if (type(error) == HttpError):\n",
    "                break\n",
    "    \n",
    "    return df, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ce144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in video id  LBraVzyYDKg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nour\\anaconda3\\envs\\mods207\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Scrapping was interrupted due to the quota being exceeded. Please wait till tomorrow!!\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "path = 'channelVideos/'  # change if you ran it on your personal environment\n",
    "all_videos = os.listdir(path)\n",
    "\n",
    "for channel in all_videos :\n",
    "    df, error = get_channel_videos(path, channel)\n",
    "    if (type(error) == HttpError):\n",
    "        warnings.warn('Scrapping was interrupted due to the quota being exceeded. Please wait till tomorrow!!')\n",
    "        day = 60*60*24\n",
    "        time.sleep(day)     \n",
    "    \n",
    "    df, error = get_channel_videos(path, channel)\n",
    "    channel_videos = 'channel' + channel.split(\"_\")[1] + 'videos.csv'\n",
    "    df.to_csv(channel_videos, index=False)\n",
    "    print('scrapped ', channel) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b14cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
